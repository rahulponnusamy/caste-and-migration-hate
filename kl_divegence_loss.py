# -*- coding: utf-8 -*-
"""Training_without_peft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GYh9At_hGfA6WhO6VQI-NISglgiVujHr
"""

# ! pip install -U accelerate
# ! pip install -U transformers
# ! pip install datasets peft

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '2'
if os.getcwd() != '/home/rahpon/projects/caste':
    os.chdir('/home/rahpon/projects/caste')
    cwd = os.getcwd()
    print(cwd)
cwd = os.getcwd()
print(cwd)


import torch
import logging
import os
import json
import pandas as pd
import numpy as np
from torch import nn
from datasets import load_dataset
from datasets import Dataset, DatasetDict
from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments,Trainer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


def classifier(model_path,output_path,lr):
  logs_dir = os.path.join(output_path,'logs')

  model_name_or_path = model_path

  tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side= "right") ###will have to be changed depending on model, left for gpt opt bloom
  if getattr(tokenizer, "pad_token_id") is None:
      tokenizer.pad_token_id = tokenizer.eos_token_id

  def tokenize_function(examples):
      # max_length=None => use the model max length (it's actually the default)
      outputs = tokenizer(examples["texts"], truncation=True, max_length=250)
      return outputs

  train_pd = pd.read_csv(r'DataSplit/train.csv')
  val_pd = pd.read_csv(r'DataSplit/val.csv')

  #Converting the pandas dataframe to HuggingFace DatasetDict format:
  train_ds = Dataset.from_pandas(train_pd)

  val_ds = Dataset.from_pandas(val_pd)

  data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding="longest")
  tokenized_train = train_ds.map(tokenize_function, batched = True)
  tokenized_val = val_ds.map(tokenize_function, batched = True)
  tokenized_train = tokenized_train.rename_column("label", "labels")
  tokenized_val = tokenized_val.rename_column("label", "labels")

  print(tokenized_train)

  model = AutoModelForSequenceClassification.from_pretrained(model_path, return_dict=True)
  #model.set_default_language("ta_IN") #only for xmod
  
  class_weights = (1-(pd.Series(tokenized_train['labels']).value_counts().sort_index()/len(tokenized_train))).values
  class_weights = torch.from_numpy(class_weights).float().to("cuda")

  print(model)
  class CustomTrainer(Trainer):
     def compute_loss(self,model,inputs,return_outputs = False):
        outputs = model(**inputs)
        logits = outputs.get("logits")
        labels = inputs.get("labels")
        #print(logits)
        #print(labels)
        # print("Logits size:", logits.size())  # or logits.shape
        # print("Labels size:", labels.size())
        labels =nn.functional.one_hot(labels, num_classes=2).float()
        #labels = labels.squeeze(dim=-1) ##new 
        loss_func = nn.KLDivLoss()
        #(weight=class_weights)
        loss = loss_func(logits,labels)
        return (loss,outputs) if return_outputs else loss

  def compute_metrics(eval_pred):
      preds, labels = eval_pred
      preds = np.argmax(preds, axis=1)
      accuracy = accuracy_score(labels, preds)
      precision = precision_score(labels, preds, average='weighted')
      recall = recall_score(labels, preds, average='weighted')
      f1 = f1_score(labels, preds, average='weighted')
      return {
          'accuracy': accuracy,
          'precision': precision,
          'recall': recall,
          'f1': f1
      }

  training_args = TrainingArguments(
    output_dir=output_path,
    learning_rate=lr,
    per_device_train_batch_size= 8,
    per_device_eval_batch_size= 8,
    num_train_epochs=5,
    weight_decay=0.01,
    evaluation_strategy="steps",  # Evaluate at specified steps
    save_strategy="steps",
    eval_steps= 50,
    save_steps = 600,
    logging_steps=50,
    load_best_model_at_end=True,
    logging_dir=logs_dir, #tensorboard
    # save_total_limit=5,
    #report_to = "wandb"
  )

  # trainer = Trainer(
  #     model=model,
  #     args=training_args,
  #     train_dataset=tokenized_train,
  #     eval_dataset = tokenized_val,
  #     tokenizer=tokenizer,
  #     data_collator=data_collator,
  #     compute_metrics = compute_metrics,
  #   )
  
  trainer = CustomTrainer(
      model=model,
      args=training_args,
      train_dataset=tokenized_train,
      eval_dataset = tokenized_val,
      tokenizer=tokenizer,
      data_collator=data_collator,
      compute_metrics = compute_metrics,
    )
  train_result = trainer.train()

  # compute train results
  metrics = train_result.metrics
  max_train_samples = len(tokenized_train)
  metrics["train_samples"] = min(max_train_samples, len(tokenized_train))

  # save train results
  trainer.log_metrics("train", metrics)
  trainer.save_metrics("train", metrics)

  trainer.save_model(training_args.output_dir)  # Saves model and associated tokenizer


  #model.save_pretrained(training_args.output_dir)

  losses= trainer.state.log_history
  #print("trainer.state.log_history: ")
  #print(losses)
  training_loss_path = os.path.join(output_path,"training_loss.json")
  eval_loss_path = os.path.join(output_path, "eval_loss.json")
  training_losses = [entry for entry in losses if 'loss' in entry]
  eval_losses = [entry for entry in losses if 'eval_loss' in entry]
  with open(training_loss_path, 'w') as f:
      json.dump(training_losses, f, indent=2)
  with open(eval_loss_path, 'w') as f:
      json.dump(eval_losses, f, indent=2)

#############################

# root_dir = 'KL_DIV'
# sub_dir = 'indic-bert'
# model_path ='ai4bharat/indic-bert'
# lr= 3e-5
# output_path = os.path.join(root_dir,sub_dir)

lr = 3e-5
root = os.path.join("saved_models/finetune/KL_DIV",str(lr))
sub_dir ="indic-bert"
output_path = os.path.join(root,sub_dir)
model_path = 'ai4bharat/indic-bert'

#############################


classifier(model_path,output_path,lr)

import torch
import pandas as pd
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from sklearn.metrics import confusion_matrix, classification_report

path = output_path
# inference_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)
tokenizer = AutoTokenizer.from_pretrained(path,local_files_only=True)
model = AutoModelForSequenceClassification.from_pretrained(path)

test_data_path = r"DataSplit/test.csv"
test_data = pd.read_csv(test_data_path)
test_texts = test_data['texts'].tolist()


tokenized_test = tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt',max_length = 250)

with torch.no_grad():
  outputs = model(**tokenized_test)

logits = outputs.logits
probs = torch.nn.functional.softmax(logits, dim=-1)
predicted_labels = torch.argmax(probs, dim=-1).tolist()

predictions_df = test_data

predictions_df['predictions'] = predicted_labels

labels = test_data['labels'].tolist()


conf_matrix = confusion_matrix(predictions_df['labels'], predictions_df['predictions'])

print(conf_matrix)


class_report = classification_report(labels, predicted_labels)
#class_report_2 = classification_report(predictions_df['labels'], predictions_df['predictions'])
print(class_report)
#print(class_report_2)

predictions_path = os.path.join(output_path,'predictions.csv')

predictions_df.to_csv(predictions_path)

metrics_path = os.path.join(root,"metrics.txt")

with open(metrics_path,'a') as f:
  f.write("learning rate: ")
  f.write(str(lr))
  f.write("\n")
  f.write(model_path)
  f.write("\n")
  f.write(class_report)
  f.write("Confusion Matrix:\n")
  np.savetxt(f,conf_matrix,fmt = "%d")

# """Saving Loss plots"""

# import matplotlib.pyplot as plt
# import json
# import os
# train_loss_path = os.path.join(output_path,"training_loss.json")
# # Load the JSON file
# with open(train_loss_path, 'r') as file:
#     data = json.load(file)

# # Extracting values for plotting
# epochs = [entry['epoch'] for entry in data]
# losses = [entry['loss'] for entry in data]

# # Plotting the training losses over epochs
# plt.plot(epochs, losses, marker='o', linestyle='-', color='b')
# plt.title('Training Loss over Epochs')
# plt.xlabel('Epoch')
# plt.ylabel('Loss')
# plt.grid(True)

# # Save the plot as an image (e.g., PNG)
# loss_plot_path = os.path.join(output_path,'training_loss_plot.png')
# plt.savefig(loss_plot_path)

"""Saving Loss plots"""

import matplotlib.pyplot as plt
import json
import os
train_loss_path = os.path.join(output_path,"training_loss.json")

# Load the JSON file
with open(train_loss_path, 'r') as file:
    data = json.load(file)

# Extracting values for plotting
epochs = [entry['epoch'] for entry in data]
losses = [entry['loss'] for entry in data]

# Plotting the training losses over epochs
plt.plot(epochs, losses, marker='o', linestyle='-', color='b')
plt.title('Training Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)


loss_plot_path = os.path.join(output_path,'training_loss_plot.png')
plt.savefig(loss_plot_path)

"""Saving Loss plots"""

import matplotlib.pyplot as plt
import json
import os
train_loss_path = os.path.join(output_path,"eval_loss.json")

# Load the JSON file
with open(train_loss_path, 'r') as file:
    data = json.load(file)

# Extracting values for plotting
epochs = [entry['epoch'] for entry in data]
losses = [entry['eval_loss'] for entry in data]

# Plotting the training losses over epochs
plt.plot(epochs, losses, marker='o', linestyle='-', color='b')
plt.title('Evaluation Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)


loss_plot_path = os.path.join(output_path,'eval_loss_plot.png')
plt.savefig(loss_plot_path)